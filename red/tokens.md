## vulnerable tokens

  - https://www.sentinelone.com/labs/inside-the-llm-understanding-ai-the-mechanics-of-modern-attacks/#:~:text=Attack%20Vector%20%7C%20Filter%20Bypass,filter%20only%20sees%20fragmented%20syntax.
  - https://www.invicti.com/blog/security-labs/first-tokens-the-achilles-heel-of-llms#:~:text=Bogdan%20Calin,%2C%20effectively%20%22jailbreaking%22%20it.
  - https://developer.nvidia.com/blog/secure-llm-tokenizers-to-maintain-application-integrity/#:~:text=Jun%2027%2C%202024,or%20corrupting%20the%20model's%20output.
  - https://hiddenlayer.com/innovation-hub/the-tokenbreak-attack/#:~:text=It%20would%20therefore%20be%20possible,you%20are%20using%20for%20protection.
  - https://arxiv.org/html/2504.20493v1#:~:text=Recent%20research%20has%20uncovered%20a,improving%20security%20in%20reasoning%20LLMs.
  - https://www.anthropic.com/research/small-samples-poison#:~:text=Large%20language%20models%20(LLMs)%20can%20be%20poisoned,*%20**Bypassing%20safety%20guardrails**%20Bypassing%20safety%20guardrails
  - https://www.usenix.org/conference/usenixsecurity25/presentation/zhang-lan#:~:text=Recent%20advancements%20in%20large%20language,an%20elevated%20risk%20of%20threat.
  - https://github.com/GraySwanAI/nanoGCG
    
